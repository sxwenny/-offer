- [卷积](#卷积)
  - [CNN的特点以及优势](#cnn的特点以及优势)
  - [卷积输出大小](#卷积输出大小)
  - [感受野计算](#感受野计算)
  - [卷积层的参数量、计算量](#卷积层的参数量计算量)
  - [转置卷积](#转置卷积)
  - [空洞卷积](#空洞卷积)
  - [分组卷积](#分组卷积)
  - [反向传播](#反向传播)
- [池化和全连接](#池化和全连接)
- [CNN的训练](#cnn的训练)
  - [数据预处理](#数据预处理)
  - [参数初始化](#参数初始化)
  - [最优化方法](#最优化方法)
  - [优化方法](#优化方法)
  - [归一化](#归一化)
  - [Dropout](#dropout)
  - [正则化](#正则化)
    - [$l_1$和$l_2$正则化](#l_1和l_2正则化)
    - [权重衰减](#权重衰减)
    - [Early Stop](#early-stop)
    - [Dropout](#dropout-1)
    - [数据增强](#数据增强)
    - [标签平滑](#标签平滑)
  - [过拟合欠拟合](#过拟合欠拟合)
  - [sigmoid和softmax](#sigmoid和softmax)
  - [分类指标](#分类指标)
  - [样本不均衡](#样本不均衡)
- [KL散度](#kl散度)
- [RNN](#rnn)
- [注意力机制](#注意力机制)
- [GAN](#gan)
# 卷积
[理解图像卷积操作的意义](https://blog.csdn.net/chaipp0607/article/details/72236892?locationNum=9&fps=1)
## CNN的特点以及优势
CNN使用范围是具有局部空间相关性的数据，比如图像、自然语言、语音。  
局部连接：可以提取局部特征。  
权值共享：减少参数数量，降低训练难度。  
降维：通过池化或卷积stride实现。  
多层次结构：将低层次的局部特征组合成为较高层次的特征。不同层级的特征可以对应不同任务。  
## 卷积输出大小
1.VALID：$\lfloor$$\frac{n_{in}-k}{s}$$\rfloor$+1 *or* $\lceil$$\frac{n_{in}-k+1}{s}$$\rceil$  
2.SAME：$\lceil$$\frac{n_{in}}{s}$$\rceil$  
3.已知padding大小：$\lfloor$$\frac{n_{in}+2p-k}{s}$$\rfloor$+1  
为空洞卷积时，有$k=d \times (k-1)+1$，则：$\lfloor$$\frac{n_{in}+2p-d \times (k-1)-1}{s}$$\rfloor$+1
## 感受野计算
[卷积神经网络物体检测之感受野大小计算](https://www.cnblogs.com/objectDetect/p/5947169.html)  
[卷积神经网络的感受野](https://zhuanlan.zhihu.com/p/44106492)  
[A guide to receptive field arithmetic for Convolutional Neural Networks](https://medium.com/mlreview/a-guide-to-receptive-field-arithmetic-for-convolutional-neural-networks-e0f514068807)  
*A receptive field of a feature can be described by its center location and its size. Within a receptive field, the closer a pixel to the center of the field, the more it contributes to the calculation of the output feature.*  
从后往前计算：  
$r_{l-1}=k_{l-1}+s_{l-1} \times (r_{l}-1)$  
$pos_{l}=s_{l} \times pos_{l+1}+(\frac{k_{l}-1}{2}-p_{l})$  
从前往后计算：  
$j_{l}=j_{l-1} \times s_{l}$  
$r_{l}=r_{l-1}+(k_{l}-1) \times j_{l-1}$  
$start_{l}=start_{l-1}+(\frac{k_{l}-1}{2}-p_{l}) \times j_{l-1}$
## 卷积层的参数量、计算量
[CNN 模型所需的计算力（flops）和参数（parameters）数量是怎么计算的？](https://www.zhihu.com/question/65305385/answer/256845252)  
[pytorch-OpCounter](https://github.com/Lyken17/pytorch-OpCounter)   
FLOPS：是floating point operations per second的缩写，意指每秒浮点运算次数，理解为计算速度。是一个衡量硬件性能的指标。  
FLOPs：是floating point operations的缩写（s表复数），意指浮点运算数，理解为计算量。可以用来衡量算法/模型的复杂度。  
**卷积层**：  
FLOPs：$(2 \times K \times K \times C_{i}) \times H \times W \times C_{o}$  
参数量：$(K \times K \times C_{i}+1) \times C_{o}$  
**全连接层**：  
FLOPs：$(2 \times I) \times O$  
参数量：$(I+1) \times O$  
卷积层大大减少了连接参数，所以在CNN中一般卷积层参数量占比小，计算量占比大，而全连接中参数量占比大，计算量占比小。所以我们需要减少网络参数、权值裁剪时主要针对全连接层；进行计算优化时，重点放在卷积层。
## 转置卷积
[反卷积(Deconvolution)、上采样(UNSampling)与上池化(UnPooling)](https://blog.csdn.net/a_a_ron/article/details/79181108)  
[Transposed Convolution, Fractionally Strided Convolution or Deconvolution](https://buptldy.github.io/2016/10/29/2016-10-29-deconv/)  
[深度学习 | 反卷积/转置卷积 的理解 transposed conv/deconv](https://blog.csdn.net/u014722627/article/details/60574260)  
转置卷积（transposed Convolutions）又名反卷积（deconvolution）或是分数步长卷积（fractially straced convolutions）。  
反卷积从字面上理解就是卷积的逆过程，值得注意的是反卷积虽然存在，但是在深度学习中并不常用。而转置卷积虽然又名反卷积，却不是真正意义上的反卷积。因为根据反卷积的数学含义，通过反卷积可以将通过卷积的输出信号，完全还原输入信号。而事实是，转置卷积只能还原shape大小，而不能还原value。你可以理解成，至少在数值方面上，转置卷积不能实现卷积操作的逆过程。所以说转置卷积与真正的反卷积有点相似，因为两者产生了相同的空间分辨率。但是又名反卷积（deconvolutions）的这种叫法是不合适的，因为它不符合反卷积的概念。  
卷积和转置卷积是尺寸对应的，卷积的输入大小与转置卷积的输出大小一致，分别可以看成下采样和上采样操作。
## 空洞卷积
[Multi-Scale Context Aggregation by Dilated Convolution 对空洞卷积（扩张卷积）、感受野的理解](https://blog.csdn.net/guvcolie/article/details/77884530?locationNum=10&fps=1)  
[如何理解空洞卷积（dilated convolution）？](https://www.zhihu.com/question/54149221)  
Dilated/Atrous convolution可以叫空洞卷积或者扩张卷积，诞生于图像分割领域。图像输入到网络中经过CNN提取特征，再经过pooling降低图像尺寸的同时增大感受野。由于图像分割是pixel−wise预测输出，所以还需要通过upsampling将变小的图像恢复到原始大小，upsampling通常是通过deconv(转置卷积)完成。因此图像分割FCN有两个关键步骤：池化操作增大感受野，upsampling操作扩大图像尺寸。这儿有个问题，就是虽然图像经过upsampling操作恢复了大小，但是很多细节还是被池化操作丢失了。  
如果不采用pooling，就无需下采样和上采样步骤了。但是这样会导致kernel的感受野变小，导致预测不精确。如果采用大的kernel话增加感受野，一来训练的参数变大，二来没有小的kernel叠加的正则作用，所以kernel size变大行不通。  
空洞卷是在不在增加参数量的条件下，既增大了感受野又不减小图像大小。
## 分组卷积
[A Tutorial on Filter Groups (Grouped Convolution)](https://blog.yani.io/filter-group-tutorial/)  
[对深度可分离卷积、分组卷积、扩张卷积、转置卷积（反卷积）的理解](https://blog.csdn.net/chaolei3/article/details/79374563)  
## 反向传播
[卷积神经网络(CNN)反向传播算法](https://www.cnblogs.com/pinard/p/6494810.html)  
[卷积神经网络(CNN)反向传播算法推导](https://zhuanlan.zhihu.com/p/61898234)  
[CNN的反向传播](https://jermmy.github.io/2017/12/16/2017-12-16-cnn-back-propagation/)  
[深度学习笔记（3）——CNN中一些特殊环节的反向传播](https://blog.csdn.net/qq_21190081/article/details/72871704)
# 池化和全连接
[全连接层的作用是什么？](https://www.zhihu.com/question/41037974)  
[Global Average Pooling全局平均池化的一点理解](https://blog.csdn.net/qq_23304241/article/details/80292859)  
激活函数用来帮助网络获得非线性特征。  
**池化作用**：保留显著特征、降低特征维度，增大kernel的感受野、提供一些平移旋转尺度不变性。  
**全连接作用**：在整个卷积神经网络中起到分类器的作用，将学到的“分布式特征表示”映射到样本标记空间。
# CNN的训练
## 数据预处理
参考书[《神经网络与深度学习》](https://nndl.github.io/)  
对于尺度敏感的模型，必须先对样本进行预处理，将各个维度的特征转换到相同的取值区间，并且消除不同特征之间的相关性，才能获得比较理想的结果。  
从理论上，神经网络应该具有尺度不变性，可以通过参数的调整来适应不同特征的尺度。但不同输入特征的尺度差异比较大时，参数初始化比较困难，梯度下降法的效率也会受到影响，从而增加训练难度。  
*归一化*（Normalization）方法泛指把数据特征转换为相同尺度的方法，比如把数据特征映射到[0, 1]或[−1, 1]区间内，或者映射为服从均值为0、方差为1的标准正态分布。常用归一化方法：  
**最小最大值归一化**（Min-Max Normalization）：$x_{new}=\frac{x-x_{min}}{x_{max}-x_{min}}$  
**标准化**（Standardization）也叫Z值归一化（Z-Score Normalization），将每一个维特征都调整为均值为0，方差为1，$x_{new}=\frac{x-\mu}{\sigma}$。  
**白化**（Whitening）是一种重要的预处理方法，用来降低输入数据特征之间的冗余性。输入数据经过白化处理后，特征之间相关性较低，并且所有特征具有相同的方差。白化的一个主要实现方式是使用PCA方法去除掉各个成分之间的相关性。
## 参数初始化
[聊一聊深度学习的weight initialization](https://zhuanlan.zhihu.com/p/25110150)  
[权重/参数初始化](https://zhuanlan.zhihu.com/p/72374385)  
不好的初始化参数会导致梯度传播问题，降低训练速度；而好的初始化参数，能够加速收敛，并且更可能找到较优解。如果权重一开始很小，信号到达最后也会很小；如果权重一开始很大，信号到达最后也会很大。在实际应用中，参数服从高斯分布或者均匀分布都是比较有效的初始化方式。  
**高斯分布**：$mean=0$  
**均匀分布**：服从$[−r,r]$间的均匀分布，$mean=\frac{a+b}{2}=0$，$var=\frac{(b-a)^{2}}{12}=\frac{r^2}{3}$。  
**Xavier初始化**的基本思想：保持输入和输出的方差一致，这样就避免了所有输出值都趋向于0。主要用于tanh，不适用于ReLU。  
$var=\frac{1}{n}$，对应于均匀分布：$r=\sqrt{\frac{3}{n}}$  
**He初始化（MSRA）**：针对ReLU的初始化方法。  
$var=\frac{2}{n}$，对应于均匀分布：$r=\sqrt{\frac{6}{n}}$    
**注**：以上的$n$为输入维度 $n_i$ 或输出维度 $n_o$ 或 $\frac{n_i+n_o}{2}$。  
**总结**：  
当前的主流初始化方式Xavier，MSRA主要是为了保持每层的输入与输出方差相等，而参数的分布采用均匀分布或高斯分布均可；  
在广泛采用BN的情况下，能够减缓对较好的网络参数初始化的依赖，使用普通的小方差的参数分布即可；  
另外，在迁移学习的情况下，优先采用预训练的模型进行参数初始化。
## 最优化方法 
[常见的几种最优化方法（梯度下降法、牛顿法、拟牛顿法、共轭梯度法等）](https://www.cnblogs.com/shixiangwan/p/7532830.html)  
**梯度下降法**：  
实现简单，缺点为靠近极小值时收敛速度减慢；直线搜索时可能会产生一些问题；可能会“之字形”地下降。  
**牛顿法**：  
优点：二阶收敛，收敛速度快；  
缺点：牛顿法是一种迭代算法，每一步都需要求解目标函数的Hessian矩阵的逆矩阵，计算比较复杂。  
**拟牛顿法**:  
求解非线性优化问题最有效的方法之一，改善牛顿法每次需要求解复杂的Hessian矩阵的逆矩阵的缺陷，它使用正定矩阵来近似Hessian矩阵的逆，从而简化了运算的复杂度。  
**共轭梯度法**：  
介于梯度下降法与牛顿法之间，仅需利用一阶导数信息，但克服了梯度下降法收敛慢的缺点，又避免了牛顿法需要存储和计算Hesse矩阵并求逆的缺点。  
## 优化方法
[从 SGD 到 Adam —— 深度学习优化算法概览(一)](https://zhuanlan.zhihu.com/p/32626442)  
[An overview of gradient descent optimization algorithms](https://ruder.io/optimizing-gradient-descent/)  
梯度下降分为批量梯度下降（Batch）、随机梯度下降（Stochastic）、小批量梯度下降（Mini-batch）。  
SGD、Momentum、Nesterov accelerated gradient、Adagrad、Adadelta、RMSprop、Adam。  
**鞍点**（saddle point）的数学含义是：目标函数在此点上的梯度（一阶导数）值为0，但从该点出发的一个方向是函数的极大值点，而在另一个方向是函数的极小值点。  
判断鞍点的一个充分条件是：函数在一阶导数为零处（驻点）的黑塞矩阵为不定矩阵。  
半正定矩阵：所有特征值为非负；半负定矩阵：所有特征值为非正；不定矩阵：特征值有正有负。
## 归一化
[Batch Normalization原理与实战](https://zhuanlan.zhihu.com/p/34879333)  
[如何理解归一化（Normalization）对于神经网络（深度学习）的帮助？](https://www.zhihu.com/question/326034346/answer/708331566)  
BN原理；手写BN；BN可以防止过拟合么；BN有哪些参数；BN的反向传播推导；BN在训练和测试的区别？  
[BatchNormalization、LayerNormalization、InstanceNorm、GroupNorm、SwitchableNorm总结](https://blog.csdn.net/liuxiao214/article/details/81037416)  
## Dropout
[理解dropout](https://blog.csdn.net/stdcoutzyx/article/details/49022443)  
## 正则化
[[Deep Learning] 正则化](https://www.cnblogs.com/maybe2030/p/9231231.html)  
参考书[《神经网络与深度学习》](https://nndl.github.io/)  
正则化（Regularization）是一类通过限制模型复杂度，从而避免过拟合，提高泛化能力的方法。在传统的机器学习中，提高泛化能力的方法主要是限制模型复杂度，如采用$l_1$和$l_2$正则化等方式。而在训练深度神经网络时，特别是在过度参数化（*指模型参数的数量远远大于训练数据的数量*）时，$l_1$和$l_2$正则化的效果往往不如浅层机器学习模型中显著。因此训练深度学习模型时，往往还会使用其他的正则化方法，如数据增强、提前停止、丢弃法、集成法等。  
### $l_1$和$l_2$正则化
[深度学习——L0、L1及L2范数](https://blog.csdn.net/zchang81/article/details/70208061)  
[机器学习中正则化项L1和L2的直观理解](https://blog.csdn.net/jinping_shi/article/details/52433975)  
[『科学计算』L0、L1与L2范数_理解](https://www.cnblogs.com/hellcat/p/7979711.html)  
机器学习中最常用的正则化方法，通过约束参数的$l_1$和$l_2$范数来减小模型在训练数据集上的过拟合现象。  
### 权重衰减
[权重衰减（weight decay）与学习率衰减（learning rate decay）](https://zhuanlan.zhihu.com/p/38709373)  
[在神经网络中weight decay起到的做用是什么？momentum呢？normalization呢？](https://www.zhihu.com/question/24529483)  
Weight Decay是一种有效的正则化方法，在每次参数更新时，引入一个衰减系数：$\theta_t=(1-\beta)\theta_{t-1}-\alpha g_t$，$g_t$为第$t$步更新时的梯度，$\alpha$为学习率，$\beta$为权重衰减系数，一般取值比较小，比如0.0005。在标准的随机梯度下降中，权重衰减正则化和$l_2$正则化的效果相同。因此，权重衰减在一些深度学习框架中通过$l_2$正则化来实现。但是，在较为复杂的优化方法（如Adam）中，权重衰减正则化和$l_2$正则化并不等价。
### Early Stop
在使用梯度下降法进行优化时，我们可以使用一个和训练集独立的样本集合，称为验证集（Validation Set），并用验证集上的错误来代替期望错误，当验证集上的错误率不再下降，就停止迭代。  
### Dropout
### 数据增强
旋转、翻转、缩放、裁剪、平移、添加噪声。  
### 标签平滑
$$p_i=\left\lbrace\begin{aligned}
&1-\frac{N-1}{N}\epsilon \quad &i=y \newline  
&\frac{\epsilon}{N} \quad &i\not=y
\end{aligned}\right.$$
## 过拟合欠拟合
[什么是，泛化能力，过拟合，欠拟合，不收敛，奥卡姆剃刀？](https://blog.csdn.net/limiyudianzi/article/details/79626702)
## sigmoid和softmax
[直观理解为什么分类问题用交叉熵损失而不用均方误差损失?](https://www.cnblogs.com/shine-lee/archive/2019/12/12/12032066.html)  
[机器学习面试之MSE与CE的区别？](https://www.jianshu.com/p/5d13bcd9d990)  
**sigmoid**  
$$y=\frac{1}{1+e^{-x}}$$
$$\frac{\partial{y}}{\partial{x}}=y(1-y)$$
**softmax**  
$$y_i=\frac{e^{x_i}}{\sum_{j=1}^{K}{e^{x_j}}}$$
$$\frac{\partial{y_i}}{\partial{x_i}}=y_i(1-y_i)$$
$$\frac{\partial{y_i}}{\partial{x_j}}=-y_i y_j$$
**Cross Entropy**
$$l=-\sum_{j=1}^{K}{q_j\log y_j}$$
$$\frac{\partial{l}}{\partial{y_i}}=-\frac{1}{y_i}$$
**MSE**
$$l=\sum_{j=1}^{K}(q_j - y_j)^2$$
$$\frac{\partial{l}}{\partial{y_i}}=2(y_i-1)$$
$$\frac{\partial{l}}{\partial{y_j}}=2y_j$$
**sigmoid+Cross Entropy**
$$\frac{\partial{l}}{\partial{x}}=\frac{\partial{l}}{\partial{y}}\frac{\partial{y}}{\partial{x}}=-\frac{1}{y}y(1-y)=y-1 \quad \quad q=1$$
$$\frac{\partial{l}}{\partial{x}}=\frac{\partial{l}}{\partial{y}}\frac{\partial{y}}{\partial{x}}=\frac{1}{1-y}y(1-y)=y \quad \quad q=0$$
**softmax+Cross Entropy**
$$\frac{\partial{l}}{\partial{x_i}}=\frac{\partial{l}}{\partial{y_i}}\frac{\partial{y_i}}{\partial{x_i}}=y_i-1$$
$$\frac{\partial{l}}{\partial{x_j}}=\frac{\partial{l}}{\partial{y_i}}\frac{\partial{y_i}}{\partial{x_j}}=y_j$$
**sigmoid+MSE**
$$\frac{\partial{l}}{\partial{x}}=\frac{\partial{l}}{\partial{y}}\frac{\partial{y}}{\partial{x}}=2(y-1)y(1-y)=-2y(1-y)^2 \quad \quad q=1$$
$$\frac{\partial{l}}{\partial{x}}=\frac{\partial{l}}{\partial{y}}\frac{\partial{y}}{\partial{x}}=2yy(1-y)=2y^2(1-y) \quad \quad q=0$$
**softmax+MSE**
$$\frac{\partial{l}}{\partial{x_i}}=\frac{\partial{l}}{\partial{y_i}}\frac{\partial{y_i}}{\partial{x_i}}+\sum_{j=1,j\not=i}^{K}\frac{\partial{l}}{\partial{y_j}}\frac{\partial{y_j}}{\partial{x_i}}=-2y_i(1-y_i)^2+\sum_{j=1,j\not=i}^{K}2y_jy_i(1-y_i)$$
$$\frac{\partial{l}}{\partial{x_j}}=\frac{\partial{l}}{\partial{y_i}}\frac{\partial{y_i}}{\partial{x_j}}+\frac{\partial{l}}{\partial{y_j}}\frac{\partial{y_j}}{\partial{x_j}}+\sum_{k=1,k\not=i,k\not=j}^{K}\frac{\partial{l}}{\partial{y_k}}\frac{\partial{y_k}}{\partial{x_j}}=-2y_i y_j(y_i-1)+2y_j^2(1-y_j)+\sum_{k=1,k\not=i,k\not=j}^{K}-2y_k^2y_j$$
## 分类指标
TP：预测为正，实际为正  
FP：预测为正，实际为负  
TN：预测为负，实际为负  
FN：预测为负，实际为正  
ACC（accuracy，准确率）：ACC = (TP+TN)/(TP+TN+FN+FP)  
P（precision精确率、精准率、查准率）：P = TP/(TP+FP)  
R（recall，召回率、查全率）： R = TP/(TP+FN)  
P-R曲线（precision-recall，查准率-查全率曲线）  
F1-score：P和R的调和平均，F1-score = 2xPxR/(P+R)  
$F_\beta$ score： $F_\beta=(1+\beta^2)\frac{P \times R}{(\beta^2 \times P)+R}$  
[机器学习基础（1）- ROC曲线理解](https://www.jianshu.com/p/2ca96fce7e81)  
TPR（true positive rate）：TPR = TP/(TP+FN)  
FPR（false positive rate）：FPR =FP/(FP+TN)  
ROC曲线（receiver operating characteristic）：横轴FPR，纵轴TPR  
AUC（area under curve）值  
## 样本不均衡
[如何解决机器学习中数据不平衡问题](https://zhaokv.com/machine_learning/2016/01/learning-from-imbalanced-data.html)
# KL散度
[KL散度的理解](https://blog.csdn.net/ericcchen/article/details/72357411)  
[如何通俗的解释交叉熵与相对熵？](https://www.zhihu.com/question/41252833)  
KL散度：$KL(P||Q)=\sum{P(x)\log\frac{P(x)}{Q(x)}}$，值$>=0$，不对称。  
JS散度：$JS(P||Q)=\frac{1}{2}KL(P(x)||\frac{P(x)+Q(x)}{2})+\frac{1}{2}KL(Q(x)||\frac{P(x)+Q(x)}{2})$，值域$[0,1]$，对称。
# RNN
[如何从RNN起步，一步一步通俗理解LSTM](https://blog.csdn.net/v_JULY_v/article/details/89894058)
# 注意力机制
[目前主流的attention方法都有哪些？](https://www.zhihu.com/question/68482809/answer/264070398)  
[模型汇总24 - 深度学习中Attention Mechanism详细介绍：原理、分类及应用](https://zhuanlan.zhihu.com/p/31547842)   
[【计算机视觉】深入理解Attention机制](https://blog.csdn.net/yideqianfenzhiyi/article/details/79422857)  
# GAN
[GAN原理学习笔记](https://zhuanlan.zhihu.com/p/27295635)
