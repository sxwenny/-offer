- [分类网络](#分类网络)
  - [LeNet](#lenet)
  - [AlexNet](#alexnet)
  - [VGGNet](#vggnet)
  - [Inception系列](#inception系列)
    - [Inception V1（GoogLeNet）](#inception-v1googlenet)
    - [Inception V2](#inception-v2)
    - [Inception V3](#inception-v3)
    - [Inception V4](#inception-v4)
  - [ResNet及变种](#resnet及变种)
    - [ResNet](#resnet)
    - [ResNet v2](#resnet-v2)
    - [ResNeXt](#resnext)
  - [DenseNet](#densenet)
- [注意力网络](#注意力网络)
  - [SENet](#senet)
  - [SKNet](#sknet)
  - [GCNet](#gcnet)
  - [Octave Convolution](#octave-convolution)
  - [GENet](#genet)
  - [PSANet](#psanet)
  - [CBAM](#cbam)
  - [BAM](#bam)
  - [ResNeSt](#resnest)
- [轻量化神经网络](#轻量化神经网络)
  - [SqueezeNet](#squeezenet)
  - [Xception](#xception)
  - [MobileNet系列](#mobilenet系列)
    - [MobileNet V1](#mobilenet-v1)
    - [MobileNet V2](#mobilenet-v2)
    - [MobileNet V3](#mobilenet-v3)
  - [ShuffleNet系列](#shufflenet系列)
    - [ShuffleNet V1](#shufflenet-v1)
    - [ShuffleNet V2](#shufflenet-v2)
- [分割网络](#分割网络)
  - [FCN](#fcn)
  - [U-Net](#u-net)
  - [DeepLab系列](#deeplab系列)
    - [DeepLabV1](#deeplabv1)
    - [DeepLabV2](#deeplabv2)
    - [DeepLabV3](#deeplabv3)
- [检测网络](#检测网络)
  - [R-CNN](#r-cnn)
  - [SPP-Net](#spp-net)
  - [Fast R-CNN](#fast-r-cnn)
  - [Faster R-CNN](#faster-r-cnn)
  - [FPN](#fpn)
  - [Mask RCNN](#mask-rcnn)
  - [YOLO系列](#yolo系列)
    - [YOLOv1](#yolov1)
    - [YOLOv2](#yolov2)
    - [YOLOv3](#yolov3)
    - [YOLOv4](#yolov4)
    - [YOLOv5](#yolov5)
  - [SSD](#ssd)
  - [Cascade RCNN](#cascade-rcnn)
  - [RetinaNet](#retinanet)
  - [DetNet](#detnet)
  - [EfficientDet](#efficientdet)
- [常见知识点](#常见知识点)
  - [检测](#检测)
    - [检测度量标准](#检测度量标准)
    - [NMS](#nms)
    - [IoU](#iou)
    - [mAP](#map)
    - [边框回归](#边框回归)
    - [Focal Loss和OHEM](#focal-loss和ohem)
    - [其他问题](#其他问题)
  - [分割](#分割)
    - [分割度量标准](#分割度量标准)
    - [mIoU](#miou)
    - [其他问题](#其他问题-1)
# 分类网络
[经典卷积神经网络总结：Inception v1\v2\v3\v4、ResNet、ResNext、DenseNet、SENet等](https://blog.csdn.net/liuxiao214/article/details/81914743)
## LeNet
卷积神经网络在图像分类任务上的第一次应用，**卷积**+池化+**卷积**+池化+**全连接**+**全连接**+**全连接输出**，卷积5X5 s1，池化2X2 s2。
## AlexNet
[AlexNet结构可视化](https://dgschwend.github.io/netscope/#/preset/alexnet)  
**卷积**+最大池化+**卷积**+最大池化+**卷积**+**卷积**+**卷积**+最大池化+**全连接**+**全连接**+**全连接输出**，卷积11X11 s4、5X5 s1、3X3 s1，池化3X3 s2。  
创新：  
1.提出直接堆叠卷积层，而不是在每个卷积层上堆叠池化层。使用Max Pooling，且步长比池化核的尺寸小。  
2.为减少过拟合，使用两个正则化技术：Dropout和数据增强。  
3.使用ReLU激活函数。  
4.在C1和C3的ReLU之后使用*Local response normalization*。抑制与当前激活程度高的神经元 处在同一位置但相邻特征图的神经元。使得不同特征图关注不同的地方，从而可以探索更广泛的特征。  
5.使用GPU加速运算。
## VGGNet
[VGG16结构可视化](https://dgschwend.github.io/netscope/#/preset/vgg-16)  
[深度学习经典卷积神经网络之VGGNet](https://blog.csdn.net/marsjhao/article/details/72955935)  
主要贡献是证明了增加网络的深度能够在一定程度上提升网络最终的性能。相比AlexNet，采用连续的几个3x3的卷积核代替AlexNet中的较大卷积核。对于给定的感受野，采用堆积的小卷积核优于采用大的卷积核，因为多层非线性层可以增加网络深度来保证学习更复杂的模式，而且参数更少。  
**优点**：1.结构简洁，整个网络都使用了同样大小的卷积核尺寸(3x3 s1)和最大池化尺寸(2x2 s2)；2.几个小滤波器卷积层的组合比一个大滤波器卷积层好；3.验证了通过不断加深网络结构可以提升性能。  
**缺点**：参数多，用了3层全连接。  
VGG16：13层卷积+3层全连接；VGG19：16层卷积+3层全连接。
## Inception系列
[inception-v1,v2,v3,v4----论文笔记](https://blog.csdn.net/weixin_39953502/article/details/80966046)
### Inception V1（GoogLeNet）
主要贡献是提出了Inception的卷积网络结构。  
**Inception Module**：  
1.有4个通道，分别进行1X1、3X3、5X5的卷积和3X3的最大池化。1)输出相同大小的特征图，可以顺利concat；2)采用大小不同的卷积核，意味着感受野的大小不同，就可以得到不同尺度的特征；3)采用比较大的卷积核即5X5，因为有些相关性可能隔的比较远，用大的卷积核才能学到此特征。  
2.因为5X5卷积核的计算量大，使用1X1卷积：1)降低维度，减少计算瓶颈；2)增加网络层数，提高网络的表达能力。  
使用Global Average Pooling代替全连接层，减少参数量，保留显著特征；  
使用两个辅助分类器，1)把梯度有效传递回去，解决梯度消失问题，加快训练；2)中间层的特征也有意义，空间位置特征比较丰富，有利于提升模型的判别力。
### Inception V2
提出Batch Normalization，加速网络训练，防止梯度消失；  
稍微改进Inception：同VGG使用2个3X3的卷积核代替5X5的卷积核，以保持相同感受野的同时减少参数，并加强非线性的表达能力。
### Inception V3
提出神经网络结构的设计和优化思路；  
改进Inception，分解卷积核尺寸：同VGG使用2个3X3的卷积核代替5X5的卷积核；进一步将nXn的卷积核分解成1Xn和nX1的卷积核（减少参数增加非线性外，可以处理更丰富的空间特征，增加特征的多样性）。  
改变降低特征图尺寸的方式：因为池化会损失大量信息，传统的卷积神经网络在进行池化之前会增加特征图的通道数，以保持网络的表达能力，但是计算量会大大增加。作者进行改进，使用两个通道，一个是卷积层，一个是池化层，两个通道生成的特征图大小一样，最后concat在一起。
### Inception V4
Inception V4：只用Inception结构，能否训练的更深；  
Inception-ResNet V1：计算量同Inception V3；  
Inception-ResNet V2：计算量同Inception V4；  
作者认为残差连接并不是深度网络所必需的，没有残差连接的网络训练起来并不困难，因为有好的初始化和BN，但是残差连接可以大大提升网络训练的速度。
## ResNet及变种
### ResNet
[ResNet到底在解决一个什么问题呢？](https://www.zhihu.com/question/64494691)  
**解决的问题**：当网络层数增加到某种程度，模型的效果将会不升反降，即深层网络发生了退化。
退化不是因为过拟合，因为深层CNN的训练误差和测试误差都很大；
梯度消失/爆炸（反向传播结果的数值大小不止取决于求导的式子，很大程度上也取决于输入的模值），而BN的作用本质上是控制每层输入的模值，可以有效解决梯度消失/爆炸。
残差学习的初衷，让模型的内部结构至少有恒等映射的能力。以保证在堆叠网络的过程中，网络至少不会因为继续堆叠而产生退化。  
**残差学习**：网络设计为H(x) = F(x) + x，直接把恒等映射作为网络的一部分，把问题转化为学习一个残差函数F(x) = H(x) – x。拟合残差比拟合恒等映射要容易很多。增加跳跃连接。注意有时维度不一致。  
**另外**，在输出引入一个输入x的恒等映射，则梯度也会对应地引入一个常数1，这样的网络不容易出现梯度值异常，在某种意义上，起到了稳定梯度的作用；  
*为什么 ResNet 不在一开始就使用 residual block，而是使用一个7×7的卷积* ？尽可能保留原始图像的信息。
### ResNet v2
[ResNetV2：ResNet深度解析](https://blog.csdn.net/lanran2/article/details/80247515)  
BN+ReLU+weight+BN+ReLU+weight  
恒等连接分支通路顺畅，优化起来更加简单；两个权重层都经过BN层得到标准化。
### ResNeXt
[ResNeXt算法详解](https://blog.csdn.net/u014380165/article/details/71667916)  
引入新维度cardinality，the size of the set of transformations。  
不从增加深度或宽度的角度，采用VGG堆叠的思想和Inception的split-transform-merge思想，用一种平行堆叠相同拓扑结构的block代替原来ResNet的三层卷积的block，可扩展性比较强，可以认为是在增加准确率的同时基本不改变或降低模型的复杂度。  
用分组卷积实现，3X3的卷积变成group参数为32的分组卷积。
## DenseNet
[DenseNet算法详解](https://blog.csdn.net/u014380165/article/details/75142664)  
每一层的输入是前面所有层的输出concat起来的。  
有效减轻梯度消失；加强特征传递；更有效地利用了特征；一定程度上较少了参数数量（特征图的通道数量少）；抗过拟合。  
concat后的输入channel比较大，在每个dense block的3X3卷积前面都包含了一个1X1的卷积操作，即***bottleneck layer***，目的是减少输入的特征图数量，既能降维减少计算量，又能融合各个通道的特征。此外在每两个dense block之间又增加了1X1的卷积操作，即***Translation layer***。  
使用dropout来随机减少分支，避免过拟合。
# 注意力网络
## SENet
[专栏 | Momenta详解ImageNet 2017夺冠架构SENet](https://www.sohu.com/a/161633191_465975)  
[SENet学习笔记](https://blog.csdn.net/xjz18298268521/article/details/79078551)  
从特征通道之间的关系入手，希望显式地建模特征通道之间的相互依赖关系。另外，没有引入一个新的空间维度来进行特征通道间的融合，而是采用了一种全新的“特征重标定”策略。具体来说，就是通过学习的方式来自动获取到每个特征通道的重要程度，然后依照这个重要程度去增强有用的特征并抑制对当前任务用处不大的特征，通俗来讲，就是让网络利用全局信息有选择的增强有益特征通道并抑制无用特征通道，从而能实现特征通道自适应校准。GAP + FC + ReLU + FC + Sigmoid  
Squeeze和Excitation两个操作：  
Squeeze：全局平均池化，使其具有全局的感受野，使得网络低层也能利用全局信息。  
Excitation：能够学习通道之间的非线性交互；学习一个非互斥的关系，与独热激活相反，这里允许强调多个通道。
## SKNet
[后ResNet时代：SENet与SKNet](https://zhuanlan.zhihu.com/p/60187262)  
[SKNet——SENet孪生兄弟篇](https://zhuanlan.zhihu.com/p/59690223)  
[再聊SENet的孪生兄弟SKNet](https://zhuanlan.zhihu.com/p/83221337?from_voters_page=true)  
用multiple scale feature汇总的information来channel-wise地指导如何分配侧重使用哪个kernel的表征。提出Selective Kernel Convolution，使用大一点的卷积核（5X5）提升精度，使用Attention操作，使用分组卷积进行trade off。  
Split：对特征图分成两路，分别使用卷积核大小为3X3和5X5的分组卷积；  
Fuse：两路相加用SE block，得到两个权重；  
Select：两个权重经过Softmax后分别和两路相乘，最后相加。
## GCNet
[GCNet：当Non-local遇见SENet](https://zhuanlan.zhihu.com/p/64988633)  
[2019 GCNet（attention机制，目标检测backbone性能提升）论文阅读笔记](https://zhuanlan.zhihu.com/p/65776424)  
从Non-local Network的角度出发，发现对于不同位置点的attention map是几乎一致的，说明non-local中每个点计算attention map存在很大的计算浪费，从而提出了简化的NL，也就是SNL。更进一步地，作者研究了和SENet的关联，基于SENet和SNL，提出了统一的框架，并结合两者优点提出了GCNet，计算量相对较小，又能很好地融合全局信息。  
GCNet充分结合了Non-local全局上下文建模能力强和SENet省计算量的优点。  
## Octave Convolution
[Octave Convolution详解](https://www.cnblogs.com/fydeblog/p/11655076.html)  
通过对数据低频信息减半从而达到加速卷积运算的目的。  
1.将卷积特征图分成了两组，一组低频，一组高频，低频特征图的大小会减半，从而可以有效减少存储以及计算量，另外，由于特征图大小减小，卷积核大小不变，感受野就变大了，可以抓取更多的上下文信息；  
2.即插即用的卷积块，可以直接替换传统的conv，减小内存和计算量。
## GENet
[Gather-Excite-Exploiting Feature Context......](https://zhuanlan.zhihu.com/p/51948385)  
将SENet中Squeeze操作中的全局平均池化替换为一个context的操作，即普通的带stride的卷积来提取上下文信息。
## PSANet
[论文阅读 - PSANet: Point-wise Spatial Attention Network for Scene Parsing](https://zhangbin0917.github.io/2018/09/15/PSANet-Point-wise-Spatial-Attention-Network-for-Scene-Parsing/)
## CBAM
[【CV中的Attention机制】简单而有效的CBAM模块](https://zhuanlan.zhihu.com/p/102035273)  
先后集成了通道注意力模块和空间注意力模块，使用MaxPool和AvgPool。
## BAM
[【CV中的attention机制】并联版的CBAM-BAM模块](https://zhuanlan.zhihu.com/p/102033063)  
空间注意力机制和通道注意力机制的并联。
## ResNeSt
[关于ResNeSt的点滴疑惑](https://zhuanlan.zhihu.com/p/133805433)  
Split-Attention Networks
# 轻量化神经网络
[轻量化神经网络模型总结：SqueezeNet、Xception、MobileNet、ShuffleNet](https://blog.csdn.net/liuxiao214/article/details/81875251)
## SqueezeNet
[轻量化网络：SqueezeNet](https://blog.csdn.net/u011995719/article/details/78908755)  
1.使用1X1卷积核代替3X3卷积核，减少参数量；  
2.通过squeeze layer限制通道数量，减少参数量；  
3.借鉴inception思想，将1x1和3x3卷积后结果进行concat；  
4.减少池化层，并将池化操作延后，给卷积层带来更大的激活层，保留更多地信息，提高准确率；  
5.使用全局平均池化代替全连接层。
## Xception
[Xception算法详解](https://blog.csdn.net/u014380165/article/details/75142710)  
Xception作为Inception v3的改进，主要是在Inception v3的基础上引入了depthwise separable convolution，在基本不增加网络复杂度的前提下提高了模型的效果。目的不在于模型压缩，而是提高性能。
## MobileNet系列
### MobileNet V1
[深度解读谷歌MobileNet](https://blog.csdn.net/t800ghb/article/details/78879612)  
深度可分离卷积depthwise separable convolutions（参考Xception）的本质是冗余信息更小的稀疏化表达。MobileNet V1在深度可分离卷积方法的基础上提供了高校模型设计的两个选择：宽度因子（width multiplie）和分辨率因子（resolution multiplier）；通过权衡大小、延迟时间以及精度，来构建规模更小、速度更快的MobileNet。
### MobileNet V2
[MobileNet V2 论文初读](https://zhuanlan.zhihu.com/p/33075914)  
主要引入了两个改动：Linear Bottleneck 和 Inverted Residual Blocks。
### MobileNet V3
[mobilenet系列之又一新成员---mobilenet-v3](https://blog.csdn.net/Chunfengyanyulove/article/details/91358187)
## ShuffleNet系列
### ShuffleNet V1
[轻量级网络--ShuffleNet论文解读](https://blog.csdn.net/u011974639/article/details/79200559)  
[ShuffleNet算法详解](https://blog.csdn.net/u014380165/article/details/75137111)  
[轻量级网络ShuffleNet v1](https://www.jianshu.com/p/29f4ec483b96)  
现有的先进basic架构如Xception和ResNeXt在小型网络模型中效率较低，因为大量的1×1卷积耗费很多计算资源，论文提出了逐点群卷积(pointwise group convolution)帮助降低计算复杂度；但是使用逐点群卷积会有幅作用，故在此基础上提出通道混洗(channel shuffle)帮助信息流通。  
核心就是用pointwise group convolution，channel shuffle和depthwise  convolution代替ResNet block的相应层构成了ShuffleNet uint，达到了减少计算量和提高准确率的目的。channel shuffle解决了多个group convolution叠加出现的边界效应，pointwise group convolution和depthwise  convolution主要减少了计算量。
### ShuffleNet V2
[ShuffleNetV2：轻量级CNN网络中的桂冠](https://zhuanlan.zhihu.com/p/48261931)  
结合理论与实验得到了4条实用的指导原则，并分析了ShuffleNet V1设计的不足，并在此基础上改进得到了ShuffleNet V2。
# 分割网络
## FCN
[图像语义分割入门+FCN/U-Net网络解析](https://zhuanlan.zhihu.com/p/31428783)  
[FCN学习:Semantic Segmentation](https://zhuanlan.zhihu.com/p/22976342)  
[【总结】图像语义分割之FCN和CRF](https://zhuanlan.zhihu.com/p/22308032)  
将CNN结构应用到图像语义分割领域并取得突出结果的开山之作。  
1.卷积化：将网络的全连接层替换成卷积层；  
2.上采样：转置卷积 / Resize (如双线性插值)；  
3.跳跃结构：将不同池化层的结果进行上采样，然后结合这些结果来优化输出。
## U-Net
[研习U-Net](https://zhuanlan.zhihu.com/p/44958351)  
首先进行Conv+Pooling下采样；然后Deconv反卷积进行上采样，concat之前的低层feature map，再卷积。  
与FCN逐点相加不同，U-Net将特征在channel维度拼接在一起。
## DeepLab系列
### DeepLabV1
[谷歌——DeepLab v1](https://blog.csdn.net/gzq0723/article/details/79634443)  
[Semantic Segmentation -- (DeepLabv1)Semantic image segmentation with deep convolutional ... CRFs论文解读](https://blog.csdn.net/u011974639/article/details/79134409)  
DeepLab是结合了深度卷积神经网络（DCNNs）和概率图模型（DenseCRFs）的方法。结合了DCNNs的识别能力和全连接的CRF的细粒度定位精度，寻求一个结合的方法，结果证明能够产生准确的语义分割结果。
### DeepLabV2
[Semantic Segmentation -- (DeepLabv2)Semantic Image Segmentation ... Fully Connected CRFs论文解读](https://blog.csdn.net/u011974639/article/details/79138653)
### DeepLabV3
[Semantic Segmentation -- (DeepLabv3)Rethinking Atrous Convolution for Semantic Image Segmentation论文解](https://blog.csdn.net/u011974639/article/details/79144773)
# 检测网络
[一文读懂目标检测：R-CNN、Fast R-CNN、Faster R-CNN、YOLO、SSD](https://blog.csdn.net/v_JULY_v/article/details/80170182)
## R-CNN
[RCNN- 将CNN引入目标检测的开山之作](https://zhuanlan.zhihu.com/p/23006190)  
SS+CNN+SVM  
将CNN方法引入目标检测领域， 大大提高了目标检测效果，代替传统目标检测使用的滑动窗口+手工设计特征，改变了目标检测领域的主要研究思路。  
1.候选区域生成： 一张图像生成1K~2K个候选区域 （采用Selective Search 方法）  
2.特征提取： 对每个候选区域，使用深度卷积网络提取特征 （CNN）  
3.类别判断： 特征送入每一类的SVM 分类器，判别是否属于该类  
4.位置精修： 使用回归器精细修正候选框位置
## SPP-Net
[SPPNet-引入空间金字塔池化改进RCNN](https://zhuanlan.zhihu.com/p/24774302)  
[原始图片中的ROI如何映射到到feature map?](https://zhuanlan.zhihu.com/p/24780433)  
ROI Pooling  
在feature map上提取ROI特征，这样就只需要在整幅图像上做一次卷积。  
1.图片映射到ROI  
2.空间金字塔池化 (Spatial Pyramid Pooling)：在feature map分成1x1，2x2，4x4三张子图，分别做max pooling，得到固定维度的输出。
## Fast R-CNN
[Fast R-CNN](https://zhuanlan.zhihu.com/p/24780395)  
SS + CNN + ROI  
1.将SVM分类和bbox回归联合起来在CNN阶段训练：把最后一层的Softmax换成两个，一个是对区域的分类Softmax（包括背景），另一个是对bounding box的微调。  
2.提出了一个RoI层，算是SPP的变种，SPP是pooling成多个固定尺度，RoI只pooling到单个固定的尺度。
## Faster R-CNN
[一文读懂Faster RCNN](https://zhuanlan.zhihu.com/p/31426458)  
[Faster R-CNN](https://zhuanlan.zhihu.com/p/24916624)  
RPN + CNN + ROI  
一个完全end-to-end的CNN目标检测模型，区域生成网络RPN+Fast R-CNN。  
1.Conv layers。首先使用一组基础的conv+relu+pooling层提取image的feature maps。该feature maps被共享用于后续RPN层和全连接层。  
2.Region Proposal Networks。RPN网络用于生成region proposals。该层通过softmax判断anchors属于positive或者negative，再利用bounding box regression修正anchors获得精确的proposals。（3种面积3种长宽比，2k scores和4k偏移量）  
3.Roi Pooling。该层收集输入的feature maps和proposals，综合这些信息后提取proposal feature maps，送入后续全连接层判定目标类别。  
4.Classification。利用proposal feature maps计算proposal的类别，同时再次bounding box regression获得检测框最终的精确位置。
## FPN
[FPN特征金字塔网络解读](https://zhuanlan.zhihu.com/p/61536443)
## Mask RCNN
[令人拍案称奇的Mask RCNN](https://zhuanlan.zhihu.com/p/37998710)
## YOLO系列
[你一定从未看过如此通俗易懂的YOLO系列（从V1到V5）模型解读！](https://mp.weixin.qq.com/s/HGp1QXPa26J3PHLXQfogng?tdsourcetag=s_pctim_aiomsg)
### YOLOv1  
[YOLOv1，YOLOv2，YOLOv3解读](https://blog.csdn.net/hancoder/article/details/87994678)  
[You Only Look Once: Unified, Real-Time Object Detection(YOLO)](https://zhuanlan.zhihu.com/p/31427164)  
核心思想是将目标检测转化为回归问题求解，并基于一个单独的end-to-end网络，完成从原始图像的输入到物体位置和类别的输出。Faster RCNN通过对Anchors的判别和修正获得检测框，而YOLO通过强行回归获得检测框。  
YOLO将图像分成S×S（S=7）个网格cell，每个网格要预测B（B=2）个边界框和C个类别概率，每个边界框包含5个预测量：(x, y, w, h, confidence)，则网络输出为S×S×(B∗5+C)。置信度定义为**Pr(Object)\*IOU**，Pr(Object)表示网格是否存在物体的中心，存在则为1，不存在则为0。xy表示预测框的中心相对于cell左上角坐标的偏移，wh相对图片宽高进行归一。  
**Loss = λcoord ×坐标预测误差 + （含object的confidence预测误差 + λnoobj ×不含object的confidence预测误差） + 类别预测误差**  
使用平方和衡量坐标误差，因为它很容易进行优化。大小框误差规模的解决：直接预测边界框宽度和高度的平方根，而不是宽度和高度。λcoord和 λnoobj分别为5和0.5，相当于背景的权重是前景物体框权重的一半，而前景的框位置的权重是置信度的5倍。  
**测试**：  
类别置信度 = 类别概率 * 置信度，包含了类别在框中出现的概率以及预测框与物体的拟合程度。得到每个box的类别置信度以后（7×7×2），设置阈值，滤掉得分低的 boxes，对保留的boxes进行 NMS 处理，就得到最终的检测结果。  
**特点**：  
1.检测速度快。将物体检测作为回归问题进行求解，整个检测网络简单，且训练只需一次完成。  
2.背景误检率低。YOLO在训练和推理过程中能看到整张图像的整体信息，而基于region proposal的物体检测方法（如Fast RCNN），只看到候选框内的局部图像信息。  
3.识别物体位置精准性差，√w和√h策略并没有完全解决定位准确度问题。  
4.召回率低，尤其是对小目标。这是因为一个网格中只预测了两个框，并且只属于一类。
### YOLOv2
[YOLO2](https://zhuanlan.zhihu.com/p/25167153)  
使用一系列的方法对YOLO进行了改进，在保持原有速度的同时提升精度得到YOLOv2；  
提出了一种目标分类与检测的联合训练方法，同时在COCO和ImageNet数据集中进行训练得到YOLO9000，实现9000多种物体的实时检测。  
**Better**  
1.Batch Normalization：使用BN对网络进行优化，对网络的每一层的输入都做了归一化，这样网络就不需要每层都去学数据的分布，让网络更快收敛，同时还消除了对其他形式的正则化的依赖，使用BN可以从模型中去掉Dropout，而不会产生过拟合。  
2.High resolution classifier：为了适应新的分辨率，YOLO v2的分类网络以448×448的分辨率先在ImageNet上进行微调，微调10个epochs，让网络有时间调整滤波器，好让其能更好的运行在新分辨率上。  
3.Convolution with anchor boxes：Faster R-CNN只用卷积层与RPN 来预测Anchor Box偏移值与置信度（前景得分），而不是直接预测坐标值。作者发现通过预测偏移量而不是坐标值能够简化问题，让神经网络学习起来更容易。所以使用Anchor Boxes来预测Bounding Boxes。  
4.Dimension clusters：之前Anchor Box的尺寸是手动选择的，为了优化，在训练集的Bounding Boxes上进行k-means聚类，来为anchor找到一个比较好的初始值。如果用标准的欧式距离，尺寸大的框比小框产生更多的错误，所以距离度量使用d(box,centroid)=1−IOU(box,centroid)。  
5.Direct location prediction：每个网格负责预测5个预测框，每个框有5个坐标 （tx，ty，tw，th，to）,to是置信度。  
6.Fine-Grained Features：在26×26的特征层上添加一个passthrough层，把浅层特征图连接到深层特征图。  
7.Multi-Scale Training：每10个Batch，网络会随机地选择一个新的图片尺寸。  
**Faster**  
1.提出基于GoogleNet的Darknet-19  
2.Training for classification预训练：在224×224上预训练后，再拿448×448的输入训练。  
3.Training for detection微调训练：匹配规则，对于训练图片的gt，由该gt中心点所在的网格cell进行预测，5个中与gt有最大IOU的那个anchor负责预测，剩余4个不与该gt匹配。  
**Stronger**  
YOLO9000是用部分监督的方式在不同训练集上进行训练，同时还能检测 9000个物体类别，并保证实时运行。
### YOLOv3  
**与YOLOv2显著区别：**  
1.darknet-19改成darknet-53，后者还提供了tiny darknet版本。  
2.类FPN结构：输出3个尺度的feature map，多尺度预测，每个尺度3个prior。  
3.残差结构。
### YOLOv4
### YOLOv5
## SSD
[SSD目标检测](https://zhuanlan.zhihu.com/p/31427288)  
**特点**  
1.从YOLO中继承了将检测转化为regression的思路，一次完成目标定位与分类  
2.基于Faster RCNN中的Anchor，提出了相似的Prior box  
3.加入基于特征金字塔（Pyramidal Feature Hierarchy）的检测方式，即在不同感受野的feature map上预测目标  
**优点**  
运行速度可以和YOLO媲美，检测精度可以和Faster RCNN媲美。  
**缺点**  
需要人工设置prior box的min_size，max_size和aspect_ratio值。  
虽然采用了pyramdial feature hierarchy的思路，但是对小目标的recall依然一般，并没有达到碾压Faster RCNN的级别。
## Cascade RCNN
## RetinaNet
[如何评价Kaiming的Focal Loss for Dense Object Detection？](https://www.zhihu.com/question/63581984)
## DetNet
## EfficientDet
[如何评价谷歌大脑的EfficientNet？](https://www.zhihu.com/question/326833457)  
[EfficientNet-可能是迄今为止最好的CNN网络](https://zhuanlan.zhihu.com/p/67834114)  
[EfficientNet论文解读](https://zhuanlan.zhihu.com/p/70369784)  
[EfficientNet：调参侠的福音（ICML 2019）](https://zhuanlan.zhihu.com/p/69349360)
# 常见知识点
## 检测
### 检测度量标准
mAP，FPS  
[目标检测的性能评价指标](https://zhuanlan.zhihu.com/p/70306015)
### NMS
[什么是非极大值抑制（NMS）](https://www.julyedu.com/question/big/kp_id/26/ques_id/2141)
### IoU
[目标检测番外篇(1)\_IoU](https://zhuanlan.zhihu.com/p/47189358)  
[目标检测中的IOU评价函数（intersection-over-union）](https://www.julyedu.com/question/big/kp_id/26/ques_id/2138)  
```python
class Solution:
    def getUnion(self , rect1 , rect2 ):
        # write code here
        x1 = max(rect1[0], rect2[0])
        y1 = max(rect1[1], rect2[1])
        x2 = min(rect1[2], rect2[2])
        y2 = min(rect1[3], rect2[3])
        w = max(x2-x1+1, 0)
        h = max(y2-y1+1, 0)
        inter = w*h
        s1 = (rect1[2]-rect1[0]+1)*(rect1[3]-rect1[1]+1)
        s2 = (rect2[2]-rect2[0]+1)*(rect2[3]-rect2[1]+1)
        return s1+s2-inter
```
### mAP
[目标检测番外篇(2)\_mAP](https://zhuanlan.zhihu.com/p/48992451)  
[目标检测中的mAP是什么含义？](https://www.zhihu.com/question/53405779)
### 边框回归
[边框回归详解](https://blog.csdn.net/zijin0802034/article/details/77685438)
### Focal Loss和OHEM
[视觉分类任务中处理不平衡问题的loss比较](https://blog.csdn.net/weixin_35653315/article/details/78327408)
### 其他问题
[请问 faster RCNN 和 SSD 中为什么用smooth L1 loss，和L2有什么区别？](https://www.zhihu.com/question/58200555/answer/621174180)  
[目标检测中的Anchor](https://zhuanlan.zhihu.com/p/55824651)  
[目标检测中的类别漏检问题该怎么解决？](https://www.zhihu.com/question/372208101)  
[Unet神经网络为什么会在医学图像分割表现好？](https://www.zhihu.com/question/269914775)
## 分割
### 分割度量标准
PA，MP，mIoU，FWIoU  
[论文笔记 |　基于深度学习的图像语义分割技术概述之5.1度量标准](https://blog.csdn.net/u014593748/article/details/71698246)
### mIoU
[MIoU 源码解析](https://tianws.github.io/skill/2018/10/30/miou/)
### 其他问题
[为什么深度学习中的图像分割要先编码再解码？](https://www.zhihu.com/question/322191738)
